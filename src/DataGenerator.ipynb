{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f619d87-a7e1-47ab-a194-4634fa25224b",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bf70e1-9693-4091-8bcf-9c54591490af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "from shapely import geometry\n",
    "import geopandas\n",
    "import shapely.wkt\n",
    "\n",
    "pd.options.display.max_colwidth = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69453844-2fdc-4a0e-9d77-053919703ced",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a61eeb-8fd0-4779-9a69-71a2e2e5d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read FIR Codes\n",
    "df = pd.read_csv('data/NA_FIR_Codes.csv')\n",
    "\n",
    "# Read Vertices\n",
    "df2 = pd.read_csv('data/vertices_20220621.csv')\n",
    "\n",
    "# Read Spaceports\n",
    "df3 = pd.read_csv ('data/spaceports_20201027.csv')\n",
    "\n",
    "# Read pickle file with topics and augmented text\n",
    "df4 = pd.read_pickle(\"data/allData.pkl\")\n",
    "\n",
    "# Read Polygon File\n",
    "df5 = pd.read_csv('data/polygon_20201027.csv')\n",
    "\n",
    "# Read in launch data\n",
    "df6 = pd.read_csv('data/launches_20201027.csv', parse_dates=['LAUNCH_DATE'])\n",
    "\n",
    "# Annotated Data\n",
    "df7 = pd.read_csv('data/HumanAnnotatedMatches_SVO_DB_20200127_pipes_noquotes.csv', encoding='UTF-8', on_bad_lines='skip', engine=\"python\", delimiter='|')\n",
    "\n",
    "# Read Basemap Shapefile\n",
    "states = geopandas.read_file('data/bound_p.shx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9139412-e2f9-4fe4-9be9-0ccd82948d4f",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50f1bc-1c06-4534-b36d-6b8157dc5b8d",
   "metadata": {},
   "source": [
    "#### Facet Filter Function\n",
    "Applies all our filters to each row of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149479ba-49f1-4da5-addb-6f7bb6a7d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function\n",
    "def getNotams(time, launch_id):\n",
    "    # Date filter\n",
    "    filterA = df4[((df4['POSSIBLE_START_DATE'] - datetime.timedelta(hours = 8)) <= time) & \n",
    "                  ((df4['POSSIBLE_END_DATE'] + datetime.timedelta(hours = 8)) >= time)]\n",
    "    \n",
    "    # Altitude filter\n",
    "    filterB = filterA[filterA['MAX_ALT'] >= 50 | filterA['MAX_ALT'].isna()]\n",
    "    \n",
    "    # FIR Code filter\n",
    "    filterC = filterB[filterB['LOCATION_CODE'].isin(df['FIR']) | filterB['LOCATION_CODE'].isna()]\n",
    "    \n",
    "    # Keyword filter\n",
    "    filterD = filterC[(filterC['TEXT'].str.contains(r'(?:\\s|^)rocket(?:\\s|$)') == True) | \n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)space(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)launch(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)missile(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)canaveral(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)kennedy(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)nasa(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)antares(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)orion(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)atlas(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)zenit(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)falcon(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)dragon(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)spaceship(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)minuteman(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)trident(?:\\s|$)') == True) |\n",
    "                      (filterC['TEXT'].str.contains(r'(?:\\s|^)unlimited(?:\\s|$)') == True)]\n",
    "    \n",
    "    # Topic filter\n",
    "    filterE = filterD[(filterD['TOPIC'] == 0) | (filterD['TOPIC'] == 7)]\n",
    "    \n",
    "    # Get IDs for Candidate NOTAMs\n",
    "    id_list, rec_ids = getIds(filterE, df5)\n",
    "\n",
    "    # Get Polygons for Candidate NOTAMs\n",
    "    P = getPolygons(id_list)\n",
    "    test2 = pd.DataFrame(P, index=['boundary']).rename_axis('polygon_id', axis=1).transpose().reset_index()\n",
    "\n",
    "    # Return if no polygons found\n",
    "    if id_list == []:\n",
    "        return \n",
    "    \n",
    "    # Get all spaceport to polygon interactions\n",
    "    interactions_df = find_interactions(sp_df_2, test2, ['SPACEPORT_REC_ID'], ['polygon_id'], 'buffer', 'boundary')\n",
    "    interactions_df['NOTAM_REC_ID'] = [rec_ids[x] for x in interactions_df['polygon_id']]\n",
    "    \n",
    "    # Get polygon intersections from data pre-topic filter\n",
    "    filterF = filterE[(filterE['NOTAM_REC_ID'].isin(interactions_df[interactions_df['SPACEPORT_REC_ID'] == launch_id]['NOTAM_REC_ID'].unique()))]\n",
    "    \n",
    "    # If intersections exist return them\n",
    "    if len(filterF['NOTAM_REC_ID'] > 0):\n",
    "        return filterF['NOTAM_REC_ID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70dfae7-5a0f-4580-b743-597b242fdbd8",
   "metadata": {},
   "source": [
    "#### Get Polygon Ids and Polygon Geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8eb2b1-6d93-40ee-b9ee-163e109bc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates WKT Polygons\n",
    "def getPolygons(id_list):\n",
    "    all_polys = {}\n",
    "\n",
    "    for poly_id in id_list:\n",
    "        tmp = df2[df2['POLYGON_ID'] == poly_id]\n",
    "\n",
    "        long = tmp['LONGITUDE'].astype(str).values\n",
    "        lat = tmp['LATITUDE'].astype(str).values\n",
    "\n",
    "        pointList = list(zip(tmp['LONGITUDE'], tmp['LATITUDE']))\n",
    "\n",
    "        if len(pointList) == 1:\n",
    "            poly = geometry.Point([pointList[0][0], pointList[0][1]])\n",
    "        elif len(pointList) == 2:\n",
    "            poly = geometry.LineString([[p[0], p[1]] for p in pointList])\n",
    "        else:\n",
    "            poly = geometry.Polygon([[p[0], p[1]] for p in pointList])\n",
    "\n",
    "        all_polys[poly_id] = poly\n",
    "        \n",
    "    return all_polys\n",
    "\n",
    "# Gets Poylgon ID from NOTAM ID\n",
    "def getIds(rec_ids, poly_df):\n",
    "    id_list = []\n",
    "    rec_id = {}\n",
    "    for row in rec_ids['NOTAM_REC_ID']:\n",
    "        for item in poly_df[poly_df['NOTAM_REC_ID'] == row]['POLYGON_ID'].values:\n",
    "            id_list.append(item)\n",
    "            rec_id[item] = row\n",
    "            \n",
    "    return id_list, rec_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f336341-480b-4168-bf85-b9d4bd36501a",
   "metadata": {},
   "source": [
    "#### Geocircle and Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4895a8-d948-4e6d-a42f-e7f08dcf3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a utility function for converting distance values using different units\n",
    "def conv_dist(distance_value, units_value):\n",
    "    # Determine the conversion factor for the specified units (meters are required for this projection)\n",
    "    if units_value == \"mi\":\n",
    "        unit_factor = 1609.344\n",
    "    elif units_value == \"km\":\n",
    "        unit_factor = 1000.0\n",
    "    elif units_value == \"ft\":\n",
    "        unit_factor = 0.3048\n",
    "    elif units_value == \"nm\":\n",
    "        unit_factor = 1852\n",
    "    elif units_value == \"m\":\n",
    "        unit_factor = 1\n",
    "    else:  # Bad units\n",
    "        unit_factor = 0\n",
    "\n",
    "    return distance_value * unit_factor\n",
    "\n",
    "#This function creates geospatial circle(s) based on center, radius and unit values in the dataset\n",
    "def gen_geocircle(input_df, key_col, center_col, radius_col, units_col):\n",
    "\n",
    "    # Convert point data to geopandas dataframe\n",
    "    working_cols = [key_col] + [center_col] + [radius_col] + [units_col]\n",
    "    return_cols = [key_col] + [\"buffer\"]\n",
    "    \n",
    "    pointsdf = input_df[working_cols]\n",
    "    gdf_pts = geopandas.GeoDataFrame(pointsdf, geometry=center_col)\n",
    "    \n",
    "    # Add CRS (start with WGS84 to match lat/lon values)\n",
    "    gdf_pts.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    #Prepare projection (North America Lambert Conformal Conic)\n",
    "    # This projection is equidistant for measuring between points.\n",
    "    # Units are in meters\n",
    "    projout = '+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs'\n",
    "\n",
    "    # Convert to Lambert projection\n",
    "    gdf_pts = gdf_pts.to_crs(projout)\n",
    "    gdf_pts[units_col] = gdf_pts[units_col].str.lower()\n",
    "    gdf_pts[\"dist\"] = 0\n",
    "\n",
    "    for pt_index,pt_row in gdf_pts.iterrows():\n",
    "        dvalue = conv_dist(pt_row[2], pt_row[3])\n",
    "\n",
    "        gdf_pts.loc[pt_index,'dist'] = dvalue\n",
    "\n",
    "    gdf_pts[\"buffer\"] = gdf_pts[center_col].buffer(gdf_pts['dist'])\n",
    "\n",
    "    gdf_circle = gdf_pts[return_cols]\n",
    "\n",
    "    gdf_circle = geopandas.GeoDataFrame(gdf_circle, geometry='buffer')\n",
    "\n",
    "    gdf_circle = gdf_circle.to_crs(epsg=4326)\n",
    "\n",
    "    return gdf_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aa7e5-835c-441f-a5c5-540981f888fc",
   "metadata": {},
   "source": [
    "#### Interactions Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f4da43-c42c-4fa9-a80e-6021346d0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function determines takes two sets of geospatial objects and determines which ones interact\n",
    "def find_interactions(geom_set1_df, geom_set2_df,\n",
    "                      set1_key_cols, set2_key_cols,\n",
    "                      set1_geometry_col, set2_geometry_col\n",
    "                      ):\n",
    "\n",
    "    set1_working_cols = set1_key_cols + [set1_geometry_col]\n",
    "    set2_working_cols = set2_key_cols + [set2_geometry_col]\n",
    "    \n",
    "    # Convert point data to geopandas dataframe\n",
    "\n",
    "    gdf1 = geom_set1_df[set1_working_cols]\n",
    "    gdf1 = geopandas.GeoDataFrame(gdf1, geometry=set1_geometry_col)\n",
    "\n",
    "    # Add CRS (start with WGS84 to match lat/lon values)\n",
    "    gdf1.set_crs(epsg=4326, inplace=True)\n",
    "    gdf1_type = gdf1.loc[0, set1_geometry_col].geom_type\n",
    "\n",
    "    #Convert polygon data to geopandas dataframe\n",
    "    gdf2 = geom_set2_df[set2_working_cols]\n",
    "    gdf2 = geopandas.GeoDataFrame(gdf2, geometry=set2_geometry_col)\n",
    "\n",
    "    # Add CRS (start with WGS84 to match lat/lon values)\n",
    "    gdf2.set_crs(epsg=4326, inplace=True)\n",
    "    gdf2_type = gdf2.loc[0, set2_geometry_col].geom_type\n",
    "\n",
    "    #If datasets are mixed (one polygon and one linestring), ensure polygons are gdf1\n",
    "    if (gdf1_type == 'LineString' or gdf1_type == 'Point') and gdf2_type == 'Polygon':\n",
    "        gdf_temp = gdf1\n",
    "        gdf1 = gdf2\n",
    "        gdf2 = gdf_temp\n",
    "        gdf2_type = gdf1_type\n",
    "        gdf1_type = 'Polygon'\n",
    "        keys_temp = set1_key_cols\n",
    "        set1_key_cols = set2_key_cols\n",
    "        set2_key_cols = keys_temp\n",
    "        geom_temp = set1_geometry_col\n",
    "        set1_geometry_col = set2_geometry_col\n",
    "        set2_geometry_col = geom_temp\n",
    "        \n",
    "    # Convert to new equidistant projection\n",
    "\n",
    "    #Prepare projection (North America Lambert Conformal Conic)\n",
    "    # This projection is equidistant for measuring between points.\n",
    "    # Units are in meters\n",
    "    projout = '+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs'\n",
    "\n",
    "    # Convert to Lambert projection\n",
    "    gdf1 = gdf1.to_crs(projout)\n",
    "\n",
    "    # Convert to Lambert projection\n",
    "    gdf2 = gdf2.to_crs(projout)\n",
    "\n",
    "    results = pd.DataFrame(columns=(set1_key_cols + set2_key_cols + ['Interaction']))\n",
    "    \n",
    "    df_index = 0\n",
    "    \n",
    "    for gdf1_index,gdf1_row in gdf1.iterrows():\n",
    "        for gdf2_index,gdf2_row in gdf2.iterrows():\n",
    "            interaction = gdf1.loc[gdf1_index, set1_geometry_col].intersects(gdf2.loc[gdf2_index, set2_geometry_col])\n",
    "            if interaction:\n",
    "                interact_str = ['Intersect']\n",
    "                results.loc[df_index] = np.concatenate((gdf1.loc[gdf1_index, set1_key_cols].values,\n",
    "                                            gdf2.loc[gdf2_index, set2_key_cols].values,\n",
    "                                            interact_str),axis=None)\n",
    "                df_index += 1\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75e1bfb-640e-4b38-9a02-7de0e9d47fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the latitude/longtitude values to geospatial points\n",
    "spaceport_df = df3\n",
    "\n",
    "spaceport_df['FacilityLocation'] = [geometry.Point(xy) for xy in zip(spaceport_df['LONGITUDE'], spaceport_df['LATITUDE'])]\n",
    "\n",
    "spaceport_df['radius'] = 50\n",
    "spaceport_df['units'] = 'nm'\n",
    "\n",
    "spaceport_df = spaceport_df[spaceport_df['SPACEPORT_REC_ID'] > 1]\n",
    "\n",
    "sp_df_2 = gen_geocircle(spaceport_df, 'SPACEPORT_REC_ID', 'FacilityLocation', 'radius', 'units')\n",
    "sp_df_2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb8b23-0f7c-4978-a9d8-0ab8a6e22150",
   "metadata": {},
   "source": [
    "### Apply Facet Filter to All NOTAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c05cc9-8c50-46d9-9ae7-d2873474c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for launches\n",
    "df6['DISCOVERED'] = df6.apply(lambda x: getNotams(x['LAUNCH_DATE'], x['SPACEPORT_REC_ID']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba66ca-23f2-4bae-93f3-7fbc1873f5d4",
   "metadata": {},
   "source": [
    "### Making Alternative Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863e36f5-fbae-4eba-88c0-82f25b9aef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches = np.unique(np.hstack(df6[~df6['DISCOVERED'].isnull().values]['DISCOVERED'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd9c90f-706c-456a-adbd-5dd6e3f57f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altitude filter\n",
    "bad_filterA = df4[df4['MAX_ALT'] < 50000]\n",
    "    \n",
    "# FIR Code filter\n",
    "bad_filterB = bad_filterA[~bad_filterA['LOCATION_CODE'].isin(df['FIR'])]\n",
    "    \n",
    "# Topic filter\n",
    "bad_filterC = bad_filterB[(bad_filterB['TOPIC'] != 0) & (bad_filterB['TOPIC'] != 7)]['NOTAM_REC_ID'].values\n",
    "    \n",
    "bad_matches = np.unique(np.hstack((bad_filterC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254881bf-b2c9-4a96-b090-6abb14b68c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(777414,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fc0774-67cc-4a79-a189-0c7ebdf4a71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f84180-b279-4a52-9b34-bfbf7a41b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('data/handmade_good_matches.csv', good_matches, delimiter=\",\")\n",
    "np.savetxt('data/handmade_bad_matches.csv', bad_matches, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
