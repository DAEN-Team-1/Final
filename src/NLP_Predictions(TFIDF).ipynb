{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698728e3-8b99-4191-b4c0-ea562a78182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack, vstack\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49bd991-46d0-47b4-adee-5387f7e1a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Matched Data (Positive)\n",
    "#matches = pd.read_csv('data/HumanAnnotatedMatches_SVO_DB_20200127_pipes_noquotes.csv', encoding='UTF-8', on_bad_lines='skip', engine=\"python\", delimiter='|' )\n",
    "#good_ids = matches['NOTAM_REC_ID']\n",
    "\n",
    "# Load Matched Data (Negative)\n",
    "#non_matches = pd.read_csv('data/HumanAnnotatedMatches_poormatches_SVO_DB_20201027.csv', encoding='windows-1252')\n",
    "#bad_ids = non_matches['NOTAM_REC_ID']\n",
    "\n",
    "# Load All Data\n",
    "all_notams = pd.read_pickle(\"data/allData.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919bf72b-7c0b-498a-b24b-e500ec45fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ids = np.ravel(pd.read_csv('data/handmade_good_matches.csv', header=None))\n",
    "bad_ids = np.ravel(pd.read_csv('data/handmade_bad_matches.csv', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b171e23-a763-46e6-a88b-fbf9893851b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_bad_ids = np.random.choice(bad_ids, 5 * len(good_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e1f8d5-d1f1-4d1b-af36-8d3397c67c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Features\n",
    "features = ['NOTAM_TYPE', 'TEXT', 'CLASSIFICATION', 'MIN_ALT', 'MAX_ALT', 'LOCATION_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f232e42-b4c8-4136-b435-c7b5d7684733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Positive NOTAM Data\n",
    "good_notams = all_notams[all_notams['NOTAM_REC_ID'].isin(good_ids)][features]\n",
    "\n",
    "# Get Negaitve NOTAM Data\n",
    "bad_notams = all_notams[all_notams['NOTAM_REC_ID'].isin(sub_bad_ids)][features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610aa45b-1289-42b5-8d15-122cef5d2019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NOTAM_TYPE', 'TEXT', 'CLASSIFICATION', 'MIN_ALT', 'MAX_ALT',\n",
       "       'LOCATION_CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Cols\n",
    "bad_notams.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddf3a70-d08e-463a-b843-682f0e113923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Label Encoders\n",
    "le = preprocessing.LabelEncoder()\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le3 = preprocessing.LabelEncoder()\n",
    "\n",
    "# Type Encoder\n",
    "all_notams['NOTAM_TYPE_ENCODE'] = le.fit_transform(all_notams['NOTAM_TYPE'])\n",
    "good_notams['NOTAM_TYPE'] = le.transform(good_notams['NOTAM_TYPE'])\n",
    "bad_notams['NOTAM_TYPE'] = le.transform(bad_notams['NOTAM_TYPE'])\n",
    "\n",
    "# Classification Encoder\n",
    "all_notams['CLASSIFICATION_ENCODE'] = le2.fit_transform(all_notams['CLASSIFICATION'])\n",
    "good_notams['CLASSIFICATION'] = le2.transform(good_notams['CLASSIFICATION'])\n",
    "bad_notams['CLASSIFICATION'] = le2.transform(bad_notams['CLASSIFICATION'])\n",
    "\n",
    "# Location Code\n",
    "all_notams['LOCATION_CODE_ENCODE'] = le3.fit_transform(all_notams['LOCATION_CODE'])\n",
    "good_notams['LOCATION_CODE'] = le3.transform(good_notams['LOCATION_CODE'])\n",
    "bad_notams['LOCATION_CODE'] = le3.transform(bad_notams['LOCATION_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea1dbb7-7787-4cfb-bb64-3b9b4d445853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOTAM_TYPE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>MIN_ALT</th>\n",
       "      <th>MAX_ALT</th>\n",
       "      <th>LOCATION_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1</td>\n",
       "      <td>standard instrument departure sanitary diego i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0</td>\n",
       "      <td>danger area notamc danger area q ybbb qrrxx iv...</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>13941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1</td>\n",
       "      <td>navigation instrument landing system runway lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>6641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>1</td>\n",
       "      <td>runway locator foot district remaining sign at...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>6641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>2</td>\n",
       "      <td>fire and rescue category will be altostratus p...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>4028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NOTAM_TYPE                                               TEXT  \\\n",
       "709            1  standard instrument departure sanitary diego i...   \n",
       "981            0  danger area notamc danger area q ybbb qrrxx iv...   \n",
       "1398           1  navigation instrument landing system runway lo...   \n",
       "1503           1  runway locator foot district remaining sign at...   \n",
       "3009           2  fire and rescue category will be altostratus p...   \n",
       "\n",
       "      CLASSIFICATION  MIN_ALT  MAX_ALT  LOCATION_CODE  \n",
       "709                1      0.0    999.0           8092  \n",
       "981                2     50.0    600.0          13941  \n",
       "1398               0      0.0    999.0           6641  \n",
       "1503               0      0.0    999.0           6641  \n",
       "3009               2      0.0    999.0           4028  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Data\n",
    "bad_notams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6257cc21-8168-4a2b-90dd-ccdfe3dc45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message Embedding\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', max_features=10000)\n",
    "\n",
    "# Fit and Transform All Data\n",
    "all_encodes = tfidf.fit_transform(all_notams['TEXT'])\n",
    "\n",
    "# Transform Positive Data\n",
    "good_encodes = tfidf.transform(good_notams['TEXT'])\n",
    "\n",
    "# Transform Negative Data\n",
    "bad_encodes = tfidf.transform(bad_notams['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e71d22-9bbf-4899-8791-23ce4fd9fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Classification Labels\n",
    "good_notams['LABEL'] = 1\n",
    "bad_notams['LABEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca193be-ef68-4f6f-bf92-b021e37accde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Features (Positive)\n",
    "positive = hstack((good_notams['NOTAM_TYPE'].array[:,None], \n",
    "                   good_encodes,\n",
    "                   good_notams['CLASSIFICATION'].array[:,None],\n",
    "                   good_notams['MIN_ALT'].array[:,None],\n",
    "                   good_notams['MAX_ALT'].array[:,None],\n",
    "                   good_notams['LOCATION_CODE'].array[:,None])).A\n",
    "\n",
    "# Combine Features (Negative)\n",
    "negative = hstack((bad_notams['NOTAM_TYPE'].array[:,None], \n",
    "                   bad_encodes,\n",
    "                   bad_notams['CLASSIFICATION'].array[:,None],\n",
    "                   bad_notams['MIN_ALT'].array[:,None],\n",
    "                   bad_notams['MAX_ALT'].array[:,None],\n",
    "                   bad_notams['LOCATION_CODE'].array[:,None])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "930db9f4-68dc-49f4-8211-5a7ffe434d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize K-Fold Split for Cross Validation\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.025, train_size=0.05, random_state=232323)\n",
    "\n",
    "# Combine Postive and Negative Data\n",
    "X = vstack((positive, negative))\n",
    "y = vstack((good_notams['LABEL'].array[:,None], bad_notams['LABEL'].array[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f04afbbd-466b-43ae-8b32-c49a7acd32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Features (All Data)\n",
    "pred = hstack((all_notams['NOTAM_TYPE_ENCODE'].array[:,None], \n",
    "                   all_encodes,\n",
    "                   all_notams['CLASSIFICATION_ENCODE'].array[:,None],\n",
    "                   all_notams['MIN_ALT'].array[:,None],\n",
    "                   all_notams['MAX_ALT'].array[:,None],\n",
    "                   all_notams['LOCATION_CODE_ENCODE'].array[:,None])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3b72424-948c-46b7-a37e-6d5137691967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Accuracy: 0.984\n",
      "Accuracy: 0.984\n",
      "Accuracy: 0.984\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation XGBoost\n",
    "for train_index, test_index in sss.split(X.toarray(), y.toarray()):\n",
    "    X_train, X_test = X.tocsr()[train_index], X.tocsr()[test_index]\n",
    "    y_train, y_test = y.tocsr()[train_index].toarray(), y.tocsr()[test_index].toarray()\n",
    "   \n",
    "    test_model = XGBClassifier()\n",
    "    test_model.fit(X_train, y_train)\n",
    "    yhat = test_model.predict(X_test)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(y_test, yhat))\n",
    "    \n",
    "# Final Model Training\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y.toarray())\n",
    "\n",
    "# Predict All NOTAMs\n",
    "all_notams['XGB_NEW'] = model.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a2527b-e953-4180-896f-3edc31f07931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NAN to 0 for models that cannot deal with NULL\n",
    "pred[np.isnan(pred)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d00330-1f2f-4565-ae4a-ac12efe0c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.984\n",
      "Accuracy: 0.968\n",
      "Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Linear Regression\n",
    "for train_index, test_index in sss.split(X.toarray(), y.toarray()):\n",
    "    X_train, X_test = X.tocsr()[train_index], X.tocsr()[test_index]\n",
    "    y_train, y_test = y.tocsr()[train_index].toarray(), y.tocsr()[test_index].toarray()\n",
    "    \n",
    "    X_train.data[np.isnan(X_train.data)] = 0\n",
    "    X_test.data[np.isnan(X_test.data)] = 0\n",
    "   \n",
    "    test_model = LogisticRegression(random_state=0, max_iter=1000, penalty='none')\n",
    "    test_model.fit(X_train, np.ravel(y_train))\n",
    "    yhat = test_model.predict(X_test)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(np.ravel(y_test), yhat))\n",
    "\n",
    "X.data[np.isnan(X.data)] = 0\n",
    "\n",
    "# Final Model Training\n",
    "model = LogisticRegression(random_state=0, max_iter=1000, penalty='none')\n",
    "model.fit(X, np.ravel(y.toarray()))\n",
    "\n",
    "# Predict All NOTAMs\n",
    "all_notams['LinReg_NEW'] = model.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f942c854-2c3c-4d04-8d07-7b14cc49e14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.832\n",
      "Accuracy: 0.832\n",
      "Accuracy: 0.832\n",
      "Accuracy: 0.832\n",
      "Accuracy: 0.832\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation SVM\n",
    "for train_index, test_index in sss.split(X.toarray(), y.toarray()):\n",
    "    X_train, X_test = X.tocsr()[train_index], X.tocsr()[test_index]\n",
    "    y_train, y_test = y.tocsr()[train_index].toarray(), y.tocsr()[test_index].toarray()\n",
    "    \n",
    "    X_train.data[np.isnan(X_train.data)] = 0\n",
    "    X_test.data[np.isnan(X_test.data)] = 0\n",
    "   \n",
    "    test_model = svm.SVC(random_state=0, C=0.1)\n",
    "    test_model.fit(X_train, np.ravel(y_train))\n",
    "    yhat = test_model.predict(X_test)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(y_test, yhat))\n",
    "    \n",
    "X.data[np.isnan(X.data)] = 0  \n",
    "\n",
    "# Final Model Training\n",
    "model = svm.SVC(random_state=0, C=0.1)\n",
    "model.fit(X, np.ravel(y.toarray()))\n",
    "\n",
    "# Predict All NOTAMs\n",
    "all_notams['SVM_NEW'] = model.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b876e306-bf4c-4751-95cb-c541c4001411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "all_notams.to_pickle(\"data/allData.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
